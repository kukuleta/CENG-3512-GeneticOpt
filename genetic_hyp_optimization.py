# -*- coding: utf-8 -*-
"""CENG 3512 ÅžB EVALUTIONARY COMP. FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BO9-CpzlV4yWYTdJ3JKr3YIdczFJ66xF

numpy==1.19.5
optuna==2.8.0
scikit-learn==0.22.2.post1
lightgbm==2.2.3
pandas==1.1.5
"""

from functools import partial

import pandas as pd
import numpy as np

from sklearn import metrics
from sklearn import datasets, model_selection, metrics, tree, ensemble, linear_model, svm

from optuna import distributions
from optuna.samplers._random import RandomSampler
from optuna._transform import _SearchSpaceTransform

import lightgbm as lgb

seed = 42
sampler = RandomSampler(seed=seed)
random_seed_gen = np.random.RandomState(seed)

X_r = pd.read_csv("sample_data/california_housing_train.csv")
X_r_test = pd.read_csv("sample_data/california_housing_test.csv")
X_train_regression, Y_train_regression, X_test_regression, Y_test_regression = X_r.loc[:, ~X_r.columns.isin(["median_house_value"])], X_r.loc[:, X_r.columns.isin(["median_house_value"])], X_r_test.loc[:, ~X_r_test.columns.isin(["median_house_value"])], X_r_test.loc[:, X_r_test.columns.isin(["median_house_value"])]

def get_discrete_intervals(parameter_search_space, bounds):

    keys = []
    intervals = []

    for name, dist, in parameter_search_space["DecisionTree"].items():
        if not "Categorical" in dist.__name__:
            intervals.append(pd.interval_range(start=bounds[name]["lower_bound"], end=bounds[name]["upper_bound"], periods=bounds[name]["bin_count"], closed="right"))
        else:
            intervals.append(bounds[name]["choices"])
            
        keys.append(name)

    return keys, intervals

def interval_to_parameters(x):
    return [(interval.left + 1, interval.right) for interval in x]
    
def get_configuration_parameters(parameter_search_space, parameter_bounds):
    keys, discrete_intervals = get_discrete_intervals(parameter_search_space, parameter_bounds)
    interval_mappings = {key: dict(zip(discrete_intervals[idx], range(len(discrete_intervals[idx])))) for idx, key in enumerate(keys)}

    return keys, discrete_intervals, interval_mappings

def sample(
        param_name: str,
        param_distribution: distributions.BaseDistribution,
        rng=random_seed_gen
    ):

        search_space = {param_name: param_distribution}
        trans = _SearchSpaceTransform(search_space)
        trans_params = rng.uniform(trans.bounds[:, 0], trans.bounds[:, 1])

        return trans.untransform(trans_params)[param_name]

def sample_parameters(parameter_keys, intervals_selected, parameter_search_space):

    parameters = [generate_search_space(parameter_keys, interval, parameter_search_space) for interval in intervals_selected]

    return parameters, intervals_selected

def get_discrete_choices(choice_list):
  return [{key: sample(key, val) for key, val in params.items()} for params in choice_list]

def discrete_rep_to_intervals(choices, mappings):
    choice_list = []
    for choice in choices:
        choice_ = []
        for idx, interval_map in enumerate(choice):
            choice_.append(mappings[idx][interval_map])
        choice_list.append(choice_)

    return np.array(choice_list)

def generate_parameters(key, param_values, params_dist):

    if not "Categorical" in params_dist[key].__name__:
        parameter_keys = ["low", "high"]

    else:
        parameter_keys = ["choices"]

    return params_dist[key](**dict(zip(parameter_keys, param_values)))


def generate_search_space(keys, intervals, params_dist):
    return {key: generate_parameters(key, params, params_dist) for key, params in zip(keys, interval_to_parameters(intervals))} # params_dist[key](**dict(zip(["low", "high"], params)))

def get_chromosome_representation(choice_list, keys, mappings):
    return [mappings[key][choice] for key, choice in zip(keys, list(choice_list))],


def evaluate_fitness(
                     X_train,
                     Y_train,
                     X_eval,
                     Y_eval,
                     learning_params,
                     #model_signature,
                     evaluation_metric_fn,
                     model_params=None,
                     fit_params=None,
                     ):
    
    if not model_params:
        model_params = {}

    if not fit_params:
       fit_params = {}


    model_params.update(learning_params)
    
    fit_params.update({"train_set": lgb.Dataset(X_train,
                                                 label=Y_train), 
                        "valid_sets": lgb.Dataset(X_eval,
                                                  label=Y_eval)})
    
    model = lgb.train(params=model_params, **fit_params)
    predictions = model.predict(X_eval)

    res = evaluation_metric_fn(predictions, Y_eval)
    return res

def initialize_discrete_population(parameter_search_space, parameter_keys, parameter_intervals, population_size):
    intervals_selected = np.concatenate([np.random.choice(interval, size=(population_size, 1)) for interval in parameter_intervals], axis=1)
    #parameters = [generate_search_space(parameter_keys, interval, parameter_search_space) for interval in intervals_selected]

    return sample_parameters(parameter_keys, intervals_selected, parameter_search_space) #parameters, intervals_selected # intervals_selected, interval_mappings

def get_mutation_probs(df, idx):
    pct_rank = df.groupby([idx])["fitness"].mean().rank(ascending=True, pct=True)
    res = (pct_rank / pct_rank.sum())

    return res.fillna(0).values

def get_roulette_selected_pop(df, round=3):
    pop = pd.DataFrame(df).copy()
    pop["fitness"] = fitness_vals
    pop["fit_score"] = (((pop["fitness"] - pop["fitness"].quantile(0.95)) + 1) ** 100) / (((pop["fitness"] - pop["fitness"].quantile(0.95)) + 1) ** 100).sum()
    pop["fit_prob"] = pop.sort_values(["fit_score"])["fit_score"].cumsum()
    masks = [np.random.random(size=(pop.shape[0])) <= pop["fit_prob"] for ix in range(round)]
    selected_pop = []
    for mask in masks:
        selected_pop += mask[mask == True].index.tolist()
    pop = pd.concat([pop, pop.loc[selected_pop]])

    return pop

def mutate_op(population, fitnesses, bin_counts, switch_prop=0.8, mutate_prob=0.9):
    pop = population.copy()
    eval_set = pd.DataFrame(pop)
    population_size, parameter_group_len = eval_set.shape
    
    eval_set["fitness"] = fitnesses
    
    mutate_probs_by_parameters = [get_mutation_probs(eval_set, col) for col in range(parameter_group_len)]
    mutate_parameter_group_std = [eval_set.groupby([col])["fitness"].std().sum() for col in eval_set.columns[~eval_set.columns.isin(["fitness"])]]
    group_level_vars = [sorted(eval_set[col].unique()) for col in range(parameter_group_len)]


    parameter_switch_probabilities =  np.random.random(size=(population_size, 1))
    mutation_probabilities = np.random.random(size=(population_size, 1))
    parameter_group_mutations = [np.random.choice(np.argsort(mutate_parameter_group_std)[:-1]) if mask else np.argmax(mutate_parameter_group_std) for mask in (parameter_switch_probabilities > switch_prop).squeeze()]

    mutations = pd.Series(parameter_group_mutations).apply(lambda x: np.random.choice(group_level_vars[x], p=mutate_probs_by_parameters[x]))

    for group in range(parameter_group_len):
        switch_mask = pd.Series(parameter_group_mutations) == group
        mutation_mask = pd.Series((parameter_switch_probabilities > mutate_prob).squeeze())
        eval_set.loc[switch_mask & mutation_mask, group] = mutations[switch_mask & mutation_mask].values
    
    return pd.concat([pd.DataFrame(pop), eval_set.drop(columns=["fitness"])]).drop_duplicates(keep="first")

def get_roulette_selected_pop(df, round=3):
    pop = pd.DataFrame(df).copy()
    pop["fitness"] = fitness_vals
    pop["fit_score"] = (((pop["fitness"] - pop["fitness"].quantile(0.95)) + 1) ** 100) / (((pop["fitness"] - pop["fitness"].quantile(0.95)) + 1) ** 100).sum()
    pop["fit_prob"] = pop.sort_values(["fit_score"])["fit_score"].cumsum()
    masks = [np.random.random(size=(pop.shape[0])) <= pop["fit_prob"] for ix in range(round)]
    selected_pop = []
    for mask in masks:
        selected_pop += mask[mask == True].index.tolist()
    pop = pd.concat([pop, pop.loc[selected_pop]])

    return pop

    #print(mutate_probs_by_parameters)

population_size = 50

parameter_search_space = {"DecisionTree": {"max_depth": distributions.IntUniformDistribution,
                                           "min_samples_split": distributions.IntUniformDistribution}}

parameter_bounds = {"max_depth": {"lower_bound": 0, "upper_bound": 8, "bin_count": 4},
                    "min_samples_split": {"lower_bound": 0, "upper_bound": 128, "bin_count": 16}}

parameter_bin_counts = np.array(list(map(lambda x: x["bin_count"], parameter_bounds.values())))
rep = np.zeros((population_size, parameter_bin_counts.sum()))


keys, discrete_intervals, interval_mappings = get_configuration_parameters(parameter_search_space, parameter_bounds)
parameters, intervals_selected = initialize_discrete_population(parameter_search_space["DecisionTree"], keys, discrete_intervals, population_size)

selected_params = np.apply_along_axis(partial(get_chromosome_representation, keys=keys, mappings=interval_mappings), axis=1, arr=intervals_selected).reshape(population_size, -1)
rep[np.arange(0, population_size).reshape(-1, 1), (selected_params + np.concatenate([np.array([0]), parameter_bin_counts[:-1]]))] = 1

model_params = {"objective": "regression",
                "max_depth": -1,
                "metric": "mse"}
fit_params = {"verbose_eval": False,
              "num_boost_round": 200}
kwargs = {"X_train": X_train_regression,
          "Y_train": Y_train_regression,
          "X_eval": X_test_regression,
          "Y_eval": Y_test_regression,
          "model_params": model_params,
          "evaluation_metric_fn": metrics.r2_score,
          "fit_params": fit_params}

fitness_vals = list(map(lambda lr_params: evaluate_fitness(**{**kwargs,
                                               **{"learning_params": lr_params}}), 
                                               get_discrete_choices(parameters)))

def evolve_population(pop, prev_fitness, model_kwargs, parameter_bin_counts=parameter_bin_counts, switch_prop=0.5, mutate_prob=0.1, k=50, parameter_keys=keys, parameter_search_space=parameter_search_space["DecisionTree"]): #keys=keys, interval_mappings=interval_mappings)

    pop = mutate_op(pop, prev_fitness, parameter_bin_counts, switch_prop=switch_prop, mutate_prob=switch_prop)
    intervals_selected = discrete_rep_to_intervals(pop.values, discrete_intervals)


    parameters, _ = sample_parameters(parameter_keys, intervals_selected, parameter_search_space)
    fitness_vals = list(map(lambda lr_params: evaluate_fitness(**{**model_kwargs,
                                                  **{"learning_params": lr_params}}), 
                                                  get_discrete_choices(parameters)))
    pop["fitness"] = fitness_vals
    pop = pop.sort_values("fitness", ascending=True).iloc[:k]

    return pop.drop(columns=["fitness"]), pop["fitness"], pop["fitness"].describe(exclude=["std"]).loc[["mean", "std"]].to_frame("value").T.reset_index(drop=True)

pd.Series(fitness_vals).describe().loc[["mean", "std"]].to_frame("value").T.reset_index(drop=True)

iteration_num = 25
df_list = []
pop, fit, res = evolve_population(selected_params, fitness_vals, kwargs, switch_prop=0.75, mutate_prob=0.75)

for _ in range(iteration_num):
    pop, fit, res = evolve_population(pop.values, fit, kwargs, switch_prop=0.85, mutate_prob=0.85)
    df_list.append(res)
    print(res)